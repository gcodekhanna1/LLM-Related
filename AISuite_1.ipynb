{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR2qLmYuSf-k",
        "outputId": "da72a5ef-1626-4fd8-d11f-52102e608f18"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "1+1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install aisuite if not already installed\n",
        "try:\n",
        "    import aisuite as ai\n",
        "except ImportError:\n",
        "    print(\"Installing aisuite...\")\n",
        "    !pip install 'aisuite[all]'\n",
        "    import aisuite as ai"
      ],
      "metadata": {
        "id": "eJV__RAl4iP7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e3e2ec1-a378-4975-d02f-ad0444f7fbcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing aisuite...\n",
            "Collecting aisuite[all]\n",
            "  Downloading aisuite-0.1.6-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting anthropic<0.31.0,>=0.30.1 (from aisuite[all])\n",
            "  Downloading anthropic-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting groq<0.10.0,>=0.9.0 (from aisuite[all])\n",
            "  Downloading groq-0.9.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.35.8 in /usr/local/lib/python3.10/dist-packages (from aisuite[all]) (1.54.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.7.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.20.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (4.12.2)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.35.8->aisuite[all]) (4.66.6)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2.23.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.26.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2.2.3)\n",
            "Downloading anthropic-0.30.1-py3-none-any.whl (863 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m863.9/863.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groq-0.9.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aisuite-0.1.6-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: aisuite, groq, anthropic\n",
            "Successfully installed aisuite-0.1.6 anthropic-0.30.1 groq-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install 'aisuite[all]'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1C-6k8HVB9n",
        "outputId": "a7192bec-f27b-4265-e6de-23a62910f9ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aisuite[all] in /usr/local/lib/python3.10/dist-packages (0.1.6)\n",
            "Requirement already satisfied: anthropic<0.31.0,>=0.30.1 in /usr/local/lib/python3.10/dist-packages (from aisuite[all]) (0.30.1)\n",
            "Requirement already satisfied: groq<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from aisuite[all]) (0.9.0)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.35.8 in /usr/local/lib/python3.10/dist-packages (from aisuite[all]) (1.54.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.7.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.20.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (4.12.2)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.35.8->aisuite[all]) (4.66.6)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2.23.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.26.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2.2.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import aisuite as ai\n",
        "import os\n",
        "from getpass import getpass\n",
        "import textwrap\n",
        "from IPython.display import Markdown, display\n",
        "import random"
      ],
      "metadata": {
        "id": "re1KAOMCTY1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_wrapped_markdown(response, width=80):\n",
        "    \"\"\"\n",
        "    Render the content of a response as Markdown in Google Colab.\n",
        "    \"\"\"\n",
        "    # Extract the content from the response\n",
        "    content = response.choices[0].message.content\n",
        "\n",
        "    # Wrap the text using textwrap while preserving Markdown formatting\n",
        "    paragraphs = content.split(\"\\n\")  # Split by lines to handle Markdown correctly\n",
        "    wrapped_content = \"\\n\".join(\n",
        "        textwrap.fill(paragraph, width=width) if paragraph.strip() else \"\" for paragraph in paragraphs\n",
        "    )\n",
        "\n",
        "    # Render the Markdown in Colab\n",
        "    display(Markdown(wrapped_content))"
      ],
      "metadata": {
        "id": "ezigLrqC5T6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = getpass('Enter your OpenAI API key: ')\n",
        "os.environ['GROQ_API_KEY'] = getpass('Enter your GROQ API key: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "BA5aW65fVpsW",
        "outputId": "09a5b32a-2e75-4fc5-ce3d-59dedfb78605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-5225f0df0f97>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'OPENAI_API_KEY'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Enter your OpenAI API key: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'GROQ_API_KEY'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Enter your GROQ API key: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    832\u001b[0m             warnings.warn(\"The `stream` parameter of `getpass.getpass` will have no effect when using ipykernel\",\n\u001b[1;32m    833\u001b[0m                     UserWarning, stacklevel=2)\n\u001b[0;32m--> 834\u001b[0;31m         return self._input_request(prompt,\n\u001b[0m\u001b[1;32m    835\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = ai.Client()"
      ],
      "metadata": {
        "id": "TSfRtAyGTd0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the models one at a time\n",
        "\n",
        "openai_4_o = \"openai:gpt-4o\"\n",
        "groq_llama_3_2_3b_preview = \"groq:llama-3.2-3b-preview\"\n",
        "groq_mixtral_8x7b_32768=\"groq:mixtral-8x7b-32768\""
      ],
      "metadata": {
        "id": "csta5RofcfMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multiple_models = [openai_4_o,groq_llama_3_2_3b_preview,groq_mixtral_8x7b_32768]"
      ],
      "metadata": {
        "id": "-kpMLGQQfBGk"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Respond in Pirate English.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "      model=groq_llama_3_2_3b_preview,\n",
        "      messages=messages,\n",
        "      temperature=0.75\n",
        "    )\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2wNhMIsc0Oq",
        "outputId": "93731609-726c-4432-f3e1-c5645aabf895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yer lookin' fer a joke, eh? Alright then, listen close and I'll spin ye a yarn:\n",
            "\n",
            "Why did the scurvy dog's parrot go to the doctor?\n",
            "\n",
            "( pause fer dramatic effect )\n",
            "\n",
            "Because it had a fowl temper, savvy?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Respond in Pirate English.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n",
        "]\n",
        "\n",
        "for model in multiple_models:\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0.75\n",
        "    )\n",
        "    print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmNsCG5WTnwI",
        "outputId": "34758954-e2d7-4c5c-bbcf-714aa4743de6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why couldn't the pirate play cards?\n",
            "\n",
            "Because he was standin' on the deck! Arrr!\n",
            "Arrrr, matey! Yer lookin' fer a joke, eh? Alright then, here be one fer ye:\n",
            "\n",
            "Why did the scurvy dog bring a ladder to the party?\n",
            "\n",
            "(pause fer dramatic effect)\n",
            "\n",
            "Because he heard the drinks be on the house!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Is Python and object oriented programming language?  Answer in 200 words or less.\"},\n",
        "]\n",
        "\n",
        "for model in multiple_models:\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            temperature=0.75\n",
        "        )\n",
        "\n",
        "        # Display the model name as a header\n",
        "        print(f\"### Model: {model}\")\n",
        "\n",
        "        # Display the wrapped response as Markdown\n",
        "        print_wrapped_markdown(response)\n",
        "        print()  # Extra line for readability\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle exceptions and print error details\n",
        "        print(f\"Error with model {model}: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        },
        "id": "2IZG25h5euWN",
        "outputId": "ff2d8720-b31a-4c2f-ffbb-61669a296f4c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Model: openai:gpt-4o\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Yes, Python is an object-oriented programming language. It supports object-\noriented programming (OOP) principles such as encapsulation, inheritance, and\npolymorphism. In Python, everything is an object, from primitive data types like\nintegers and strings to more complex data structures like lists and\ndictionaries.\n\nPython allows the creation of classes, which serve as blueprints for objects. A\nclass can have attributes (data members) and methods (functions) that define the\nbehavior of the objects instantiated from it. This encapsulation of data and\nbehavior into objects promotes reusable and modular code, which is a core tenet\nof OOP.\n\nPython's flexibility enables developers to leverage multiple paradigms,\nincluding procedural and functional programming, alongside object-oriented\nprogramming. However, its robust support for OOP makes it an excellent choice\nfor building large and complex systems that require clear structure and\nmaintainability.\n\nMoreover, Python's dynamic nature allows for easy modification and extension of\nobjects and classes, further enhancing its suitability for rapid development and\nprototyping. The language’s simplicity and readability make it accessible,\nallowing developers of varying expertise to effectively utilize OOP concepts in\nPython to create scalable and efficient software solutions."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### Model: groq:llama-3.2-3b-preview\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Yes, Python is an object-oriented programming (OOP) language. It was designed\nwith OOP principles in mind and supports most of the OOP concepts. Some key\nfeatures that make Python an OOP language include:\n\n1. **Classes and Objects**: Python's syntax allows for the creation of classes\nand objects, which are used to define and manipulate data.\n2. **Inheritance**: Python's classes can inherit properties and methods from\nparent classes, promoting code reuse and hierarchy.\n3. **Polymorphism**: Python supports method overloading and operator\noverloading, allowing for functions to behave in different ways depending on the\ninput.\n4. **Encapsulation**: Python's classes can encapsulate data and behavior, hiding\ninternal implementation details from the outside world.\n\nPython's OOP capabilities are well-documented and widely used in various\napplications, including web development, scientific computing, and data\nanalysis. Many Python libraries and frameworks, such as Django and NumPy, rely\nheavily on OOP principles to provide efficient and scalable solutions."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### Model: groq:mixtral-8x7b-32768\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Yes, Python is indeed an object-oriented programming language. Object-oriented\nprogramming (OOP) is a programming paradigm that uses \"objects\" to design\napplications and software. These objects are instances of classes, which can\ninclude both data (attributes) and functions (methods).\n\nPython supports all the main principles of OOP, including encapsulation,\ninheritance, and polymorphism. Encapsulation is the practice of bundling data\nand functions that operate on that data into a single unit, which helps to keep\nthe data hidden and protected. Inheritance is the ability of a class to inherit\nattributes and methods from another class, which promotes code reuse and a clear\nclass hierarchy. Polymorphism is the ability of a class to take on many forms,\nmeaning that a single interface can be used to represent different underlying\nforms.\n\nIn Python, everything is an object, including built-in data types like integers,\nstrings, and lists. This makes it easy to write object-oriented code in Python,\nas you can define your own classes and use them alongside built-in types.\n\nPython's syntax is also very concise and easy to read, which makes it a great\nlanguage for learning OOP. You can define classes and create objects using\nsimple, straightforward syntax, and you can use inheritance and polymorphism to\ncreate powerful, reusable code.\n\nOverall, Python is a powerful and flexible object-oriented programming language\nthat is well-suited for a wide range of applications and projects."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_prompt_with_models(prompt, models, client, temperature=0.75):\n",
        "    \"\"\"\n",
        "    Randomly selects models for answering, improving, and checking the accuracy of a prompt.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The user prompt to be processed.\n",
        "        models (list): A list of models to be used.\n",
        "        client: The AI client for generating responses.\n",
        "        temperature (float): The temperature for response generation.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    # Select 3 random models from the input list\n",
        "    random_model_list = random.sample(models, k=3)\n",
        "\n",
        "    # Assign roles to the models\n",
        "    model1, model2, model3 = random_model_list\n",
        "\n",
        "    print(f\"Selected models:\\n1. Answer: {model1}\\n2. Improve: {model2}\\n3. Check Accuracy: {model3}\\n\")\n",
        "\n",
        "    # Step 1: Generate the answer\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "    response1 = client.chat.completions.create(\n",
        "        model=model1,\n",
        "        messages=messages,\n",
        "        temperature=temperature\n",
        "    )\n",
        "    answer = response1.choices[0].message.content\n",
        "    print(f\"Model ({model1}) - Initial Answer:\\n\")\n",
        "    display(Markdown(textwrap.fill(answer, width=80)))\n",
        "\n",
        "    # Step 2: Improve the answer\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an editor. Improve the following answer:\"},\n",
        "        {\"role\": \"user\", \"content\": answer}\n",
        "    ]\n",
        "    response2 = client.chat.completions.create(\n",
        "        model=model2,\n",
        "        messages=messages,\n",
        "        temperature=temperature\n",
        "    )\n",
        "    improved_answer = response2.choices[0].message.content\n",
        "    print(f\"\\nModel ({model2}) - Improved Answer:\\n\")\n",
        "    display(Markdown(textwrap.fill(improved_answer, width=80)))\n",
        "\n",
        "    # Step 3: Check the accuracy of the improved answer\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a fact-checker. Check the accuracy of the following answer:\"},\n",
        "        {\"role\": \"user\", \"content\": improved_answer}\n",
        "    ]\n",
        "    response3 = client.chat.completions.create(\n",
        "        model=model3,\n",
        "        messages=messages,\n",
        "        temperature=temperature\n",
        "    )\n",
        "    accuracy_check = response3.choices[0].message.content\n",
        "    print(f\"\\nModel ({model3}) - Accuracy Check:\\n\")\n",
        "    display(Markdown(textwrap.fill(accuracy_check, width=80)))\n",
        "\n",
        "    # Return the random model list\n",
        "    return random_model_list"
      ],
      "metadata": {
        "id": "EXkfSFSqDIt_"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Explain why the sky is blue in 50 words or less.\"\n",
        "process_prompt_with_models(prompt, multiple_models, client)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "O5NQ2450DbW4",
        "outputId": "12367c33-a501-4902-f928-0d9a5b0f4aa2"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected models:\n",
            "1. Answer: groq:llama-3.2-3b-preview\n",
            "2. Improve: openai:gpt-4o\n",
            "3. Check Accuracy: groq:mixtral-8x7b-32768\n",
            "\n",
            "Model (groq:llama-3.2-3b-preview) - Initial Answer:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The sky appears blue due to a phenomenon called Rayleigh scattering, where\nshorter (blue) wavelengths of sunlight are scattered more by the tiny molecules\nof gases in the atmosphere, resulting in the scattered blue light being visible\nfrom our perspective."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model (openai:gpt-4o) - Improved Answer:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The sky appears blue because of a phenomenon known as Rayleigh scattering. This\noccurs when sunlight interacts with the small molecules of gases in Earth's\natmosphere. Shorter wavelengths of light, such as blue, are scattered more\neffectively than longer wavelengths like red. As a result, the scattered blue\nlight is more visible to us, making the sky appear blue from our perspective."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model (groq:mixtral-8x7b-32768) - Accuracy Check:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The answer is accurate. The blue color of the sky is indeed a result of Rayleigh\nscattering, which describes how shorter wavelengths of light, such as blue and\nviolet, are scattered more efficiently than longer wavelengths like red and\nyellow when they encounter particles much smaller than the wavelength of light,\nsuch as molecules in the Earth's atmosphere.  Although violet light is scattered\nmore than blue light, the sky appears blue, not violet, to our eyes because our\nvision is more sensitive to blue light and because sunlight reaches us with less\nviolet light to start with. Additionally, some of the violet light gets absorbed\nby the ozone layer in the Earth's atmosphere. This combination of factors causes\nthe sky to predominantly scatter and appear blue."
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['groq:llama-3.2-3b-preview', 'openai:gpt-4o', 'groq:mixtral-8x7b-32768']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yu1Nt86hKB5c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}